# GO SERVICE

GO_PORT=8080

POSTGRES_HOST=postgres
POSTGRES_PORT=5432
POSTGRES_DB=loresmith
POSTGRES_USER=loresmithuser
POSTGRES_PASSWORD=your_postgres_password_here

TEST_POSTGRES_HOST=localhost
TEST_POSTGRES_PORT=5433
TEST_POSTGRES_DB=loresmith_test
TEST_POSTGRES_USER=testuser
TEST_POSTGRES_PASSWORD=testpass

REDIS_HOST=redis
REDIS_PORT=6379

GRPC_HOST=python-service
GRPC_PORT=50051

JWT_SECRET=replace_with_a_strong_random_secret

# PYTHON SERVICE

RABBITMQ_HOST=rabbitmq
RABBITMQ_PORT=5672
RABBITMQ_USER=loresmith
RABBITMQ_PASS=loresmith

# AI PROVIDER CONFIGURATION

# Choose between local (free) or cloud (paid) AI generation
# 
# Option 1: Local Generation (free, slower, runs on your machine)
# - Set AI_PROVIDER=local
# - Requires Ollama installed: https://ollama.com/download
# - Download a model: ollama pull llama3.1:8b
# - No API costs, unlimited usage
# - Runs on your GPU/CPU
#
# Option 2: Cloud Generation (paid, faster, uses OpenRouter API)
# - Set AI_PROVIDER=openrouter
# - Requires OpenRouter API key: https://openrouter.ai/
# - Costs money per request (GPT-4: ~$0.13 per character)
# - Higher quality, faster responses

# AI Provider: "local" for Ollama (free) or "openrouter" for cloud (paid)
AI_PROVIDER=local

# OpenRouter API Key (only needed if AI_PROVIDER=openrouter)
OPENROUTER_API_KEY=your_openrouter_api_key_here

# Local Model Configuration (only used if AI_PROVIDER=local)
# Popular models: llama3.1:8b, mistral:7b, gemma2:9b
LOCAL_MODEL=llama3.1:8b

LOCAL_EMBEDDING_MODEL=nomic-embed-text

# Ollama API URL (default works for local installation)
# Use http://host.docker.internal:11434 when running in Docker
# Use http://localhost:11434 when running outside Docker
OLLAMA_URL=http://host.docker.internal:11434

OPENROUTER_EMBEDDING_MODEL=text-embedding-3-small
OPENROUTER_EMBEDDING_BASE_URL=https://openrouter.ai/api/v1

# Langfuse API Keys
LANGFUSE_PUBLIC_KEY=pk-lf-your-public-key-here
LANGFUSE_SECRET_KEY=sk-lf-your-secret-key-here
LANGFUSE_HOST=https://cloud.langfuse.com
LANGFUSE_ENABLED=true

ENABLE_IMAGE_GENERATION=true
IMAGE_PROVIDER = local
REPLICATE_API_TOKEN = your_replicate_api_token_here
AUTOMATIC1111_URL=http://host.docker.internal:7860

# R2 Storage Configuration (Cloudflare R2 - S3-compatible)
# Get these from: Cloudflare Dashboard → R2 → Manage R2 API Tokens
AWS_ACCESS_KEY_ID=your_r2_access_key_id_here
AWS_SECRET_ACCESS_KEY=your_r2_secret_access_key_here
AWS_ENDPOINT_URL=https://your_account_id.r2.cloudflarestorage.com
R2_BUCKET_NAME=loresmith-portraits
R2_PUBLIC_URL=https://pub-your_bucket_id.r2.dev


# FRONTEND SERVICE (.env.local)
GOOGLE_CLIENT_ID=your_google_client_id_here
GOOGLE_CLIENT_SECRET=your_google_client_secret_here
NEXTAUTH_URL=http://localhost:3000
NEXTAUTH_SECRET=your_nextauth_secret_here # Long string for security
NEXT_PUBLIC_API_URL=http://localhost:8080


POSTGRES_HOST=postgres
POSTGRES_PORT=5432
POSTGRES_DB=loresmith
POSTGRES_USER=loresmithuser
POSTGRES_PASSWORD=your_postgres_password_here

TEST_POSTGRES_HOST=localhost
TEST_POSTGRES_PORT=5433
TEST_POSTGRES_DB=loresmith_test
TEST_POSTGRES_USER=testuser
TEST_POSTGRES_PASSWORD=testpass

JWT_SECRET=replace_with_a_strong_random_secret

REDIS_HOST=redis
REDIS_PORT=6379
REDIS_DB=0

# AI PROVIDER CONFIGURATION

# Choose between local (free) or cloud (paid) AI generation
# 
# Option 1: Local Generation (free, slower, runs on your machine)
# - Set AI_PROVIDER=local
# - Requires Ollama installed: https://ollama.com/download
# - Download a model: ollama pull llama3.1:8b
# - No API costs, unlimited usage
# - Runs on your GPU/CPU
#
# Option 2: Cloud Generation (paid, faster, uses OpenRouter API)
# - Set AI_PROVIDER=openrouter
# - Requires OpenRouter API key: https://openrouter.ai/
# - Costs money per request (GPT-4: ~$0.13 per character)
# - Higher quality, faster responses

# AI Provider: "local" for Ollama (free) or "openrouter" for cloud (paid)
AI_PROVIDER=local

# OpenRouter API Key (only needed if AI_PROVIDER=openrouter)
OPENROUTER_API_KEY=your_openrouter_api_key_here

# Local Model Configuration (only used if AI_PROVIDER=local)
# Popular models: llama3.1:8b, mistral:7b, gemma2:9b
LOCAL_MODEL=llama3.1:8b

# Ollama API URL (default works for local installation)
# Use http://host.docker.internal:11434 when running in Docker
# Use http://localhost:11434 when running outside Docker
OLLAMA_URL=http://host.docker.internal:11434

# LANGFUSE CONFIGURATION

# Sign up for Langfuse: https://langfuse.com/
# Create a Postgres database for Langfuse (e.g. "langfuse")
# Add the following environment variables:
LANGFUSE_NEXTAUTH_SECRET=your-random-secret-here-min-32-chars
LANGFUSE_SALT=another-random-string-here

# Langfuse API Keys (for Python service to connect)
LANGFUSE_PUBLIC_KEY=pk-lf-your-public-key-here
LANGFUSE_SECRET_KEY=sk-lf-your-secret-key-here
LANGFUSE_HOST=https://cloud.langfuse.com
LANGFUSE_ENABLED=true
